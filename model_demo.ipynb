{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff80fdc369c276cc",
   "metadata": {},
   "source": [
    "# Dough Flow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a59de97c7becc",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81cd74b7353b7b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T03:21:44.202608Z",
     "start_time": "2025-10-12T03:21:44.184106Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_set = pd.read_csv('dataset/bakery_sales_revised.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "154664b4582beaee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T03:21:45.339454Z",
     "start_time": "2025-10-12T03:21:45.331109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       transaction           item           date_time period_day  \\\n",
      "0                1          Bread 2016-10-30 09:58:00    morning   \n",
      "1                2   Scandinavian 2016-10-30 10:05:00    morning   \n",
      "2                2   Scandinavian 2016-10-30 10:05:00    morning   \n",
      "3                3  Hot chocolate 2016-10-30 10:07:00    morning   \n",
      "4                3            Jam 2016-10-30 10:07:00    morning   \n",
      "...            ...            ...                 ...        ...   \n",
      "20502         9682         Coffee 2017-04-09 14:32:00  afternoon   \n",
      "20503         9682            Tea 2017-04-09 14:32:00  afternoon   \n",
      "20504         9683         Coffee 2017-04-09 14:57:00  afternoon   \n",
      "20505         9683         Pastry 2017-04-09 14:57:00  afternoon   \n",
      "20506         9684      Smoothies 2017-04-09 15:04:00  afternoon   \n",
      "\n",
      "      weekday_weekend        date  day_of_week  month  year  \n",
      "0             weekend  2016-10-30            6     10  2016  \n",
      "1             weekend  2016-10-30            6     10  2016  \n",
      "2             weekend  2016-10-30            6     10  2016  \n",
      "3             weekend  2016-10-30            6     10  2016  \n",
      "4             weekend  2016-10-30            6     10  2016  \n",
      "...               ...         ...          ...    ...   ...  \n",
      "20502         weekend  2017-04-09            6      4  2017  \n",
      "20503         weekend  2017-04-09            6      4  2017  \n",
      "20504         weekend  2017-04-09            6      4  2017  \n",
      "20505         weekend  2017-04-09            6      4  2017  \n",
      "20506         weekend  2017-04-09            6      4  2017  \n",
      "\n",
      "[20507 rows x 9 columns]\n",
      "        transaction                      date_time   day_of_week  \\\n",
      "count  20507.000000                          20507  20507.000000   \n",
      "mean    4976.202370  2017-01-18 01:31:08.552201728      3.367777   \n",
      "min        1.000000            2016-10-30 09:58:00      0.000000   \n",
      "25%     2552.000000            2016-12-03 14:28:00      2.000000   \n",
      "50%     5137.000000            2017-01-22 11:18:00      4.000000   \n",
      "75%     7357.000000            2017-02-28 16:00:00      5.000000   \n",
      "max     9684.000000            2017-04-09 15:04:00      6.000000   \n",
      "std     2796.203001                            NaN      1.968124   \n",
      "\n",
      "              month          year  \n",
      "count  20507.000000  20507.000000  \n",
      "mean       5.860535   2016.602867  \n",
      "min        1.000000   2016.000000  \n",
      "25%        2.000000   2016.000000  \n",
      "50%        3.000000   2017.000000  \n",
      "75%       11.000000   2017.000000  \n",
      "max       12.000000   2017.000000  \n",
      "std        4.542511      0.489316  \n"
     ]
    }
   ],
   "source": [
    "# rename some columns\n",
    "data_set = data_set.rename(columns = {\n",
    "    \"Transaction\" : \"transaction\",\n",
    "    \"Item\" : \"item\"\n",
    "})\n",
    "\n",
    "print(data_set)\n",
    "print(data_set.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "675cb76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most sold items  ['Coffee', 'Bread', 'Tea', 'Cake', 'Pastry']\n",
      "\n",
      "Training model for Coffee\n",
      "Mean absolute error for Coffee: 4.998387096774194\n",
      "Root mean square error for Coffee: 6.797072187536385\n",
      "Forecasted quantity for Coffee for the next day: 29\n",
      "\n",
      "Training model for Bread\n",
      "Mean absolute error for Bread: 5.276129032258065\n",
      "Root mean square error for Bread: 6.3950888414722264\n",
      "Forecasted quantity for Bread for the next day: 18\n",
      "\n",
      "Training model for Tea\n",
      "Mean absolute error for Tea: 2.8545161290322585\n",
      "Root mean square error for Tea: 3.3575101181602403\n",
      "Forecasted quantity for Tea for the next day: 8\n",
      "\n",
      "Training model for Cake\n",
      "Mean absolute error for Cake: 3.3609677419354838\n",
      "Root mean square error for Cake: 4.066753877083563\n",
      "Forecasted quantity for Cake for the next day: 4\n",
      "\n",
      "Training model for Pastry\n",
      "Mean absolute error for Pastry: 2.56741935483871\n",
      "Root mean square error for Pastry: 3.274845970275893\n",
      "Forecasted quantity for Pastry for the next day: 6\n"
     ]
    }
   ],
   "source": [
    "data_set['date_time'] = pd.to_datetime(data_set['date_time'])\n",
    "\n",
    "# extracting date features \n",
    "data_set['date'] = data_set['date_time'].dt.date\n",
    "data_set['day_of_week'] = data_set['date_time'].dt.dayofweek\n",
    "data_set['month'] = data_set['date_time'].dt.month\n",
    "data_set['year'] = data_set['date_time'].dt.year\n",
    "\n",
    "# get daily counts of each item \n",
    "daily_sales = data_set.groupby(['date', 'item']).size().reset_index(name='quantity')\n",
    "\n",
    "# get the top 5 most best sellers\n",
    "top_5_items = daily_sales.groupby('item')['quantity'].sum().nlargest(5).index\n",
    "\n",
    "print(\"Top 5 most sold items \", top_5_items.tolist())\n",
    "\n",
    "# Filter only 5 best sellers \n",
    "daily_sales_top_items = daily_sales[daily_sales['item'].isin(top_5_items)]\n",
    "\n",
    "# Pivot table to make items as columns and date as rows with values as quantity\n",
    "daily_sales_pivot = daily_sales_top_items.pivot_table(index='date', columns='item', values='quantity').fillna(0)\n",
    "\n",
    "# Feature setting\n",
    "daily_sales_pivot.index = pd.to_datetime(daily_sales_pivot.index)\n",
    "daily_sales_pivot['day_of_week'] = daily_sales_pivot.index.dayofweek\n",
    "daily_sales_pivot['month'] = daily_sales_pivot.index.month\n",
    "daily_sales_pivot['year'] = daily_sales_pivot.index.year\n",
    "daily_sales_pivot['day_of_year'] = daily_sales_pivot.index.dayofyear\n",
    "daily_sales_pivot['week_of_year'] = daily_sales_pivot.index.isocalendar().week.astype(int)\n",
    "\n",
    "# Preparing data for modeling each item \n",
    "for item in top_5_items:\n",
    "    print(\"\\nTraining model for\", item)\n",
    "\n",
    "    item_df = daily_sales_pivot[[item, 'day_of_week', 'month', 'year', 'day_of_year', 'week_of_year']].copy()\n",
    "    item_df.rename(columns = {item: 'quantity'}, inplace = True)\n",
    "\n",
    "    # setting lag features\n",
    "    for i in range(1,8):\n",
    "        item_df[f\"lag_{i}\"] = item_df['quantity'].shift(i)\n",
    "\n",
    "    item_df.dropna(inplace=True)\n",
    "\n",
    "    # Split data\n",
    "    X = item_df.drop('quantity', axis=1)\n",
    "    y = item_df['quantity']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle= False)\n",
    "\n",
    "    # Model training\n",
    "    model = RandomForestRegressor(n_estimators=100,random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation \n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, predictions) # mean absolute error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions)) # root mean square error\n",
    "    print(f\"Mean absolute error for {item}: {mae}\")\n",
    "    print(f\"Root mean square error for {item}: {rmse}\")\n",
    "\n",
    "    # Forecasting the next day\n",
    "    last_day_features = X.iloc[-1:].copy()\n",
    "    last_day_features['day_of_week'] = (last_day_features['day_of_week'] + 1) % 7\n",
    "    last_day_features['day_of_year'] = last_day_features['day_of_year'] + 1\n",
    "    if last_day_features['day_of_year'].iloc[0] > 365:\n",
    "        last_day_features['day_of_year'] = 1\n",
    "        last_day_features['year'] = last_day_features['year'] + 1\n",
    "        last_day_features['month'] = 1\n",
    "\n",
    "    last_day_sales = y.iloc[-1]\n",
    "    for i in range(1,8):\n",
    "        if i == 1:\n",
    "            last_day_features[f\"lag_{i}\"] = last_day_sales\n",
    "        else: \n",
    "            last_day_features[f'lag_{i}'] = X.iloc[-1][f'lag_{i-1}']\n",
    "\n",
    "    next_day_forecast = model.predict(last_day_features)\n",
    "    print(f'Forecasted quantity for {item} for the next day: {int(round(next_day_forecast[0]))}')\n",
    "\n",
    "    # Plot feature importances\n",
    "    feature_importances = pd.Series(model.feature_importances_, index = X.columns)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=feature_importances, y = feature_importances.index)\n",
    "    plt.title(f'Feature importances for {item}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'feature_importance_{item.replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(y_test.index, y_test.values, label = 'Actual')\n",
    "    plt.plot(y_test.index, predictions, label = 'Forecasted')\n",
    "    plt.title(f'Actual vs Forecasted Sales for {item}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Quantity Sold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'prediction_vs_actual_{item.replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
